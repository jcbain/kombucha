{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow import keras\n",
    "\n",
    "sys.path.append('../src/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessors.pretrained_embeddings import Pretrained\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_embeds = Pretrained(200, 'glove27b')\n",
    "pre_embeds.create_from_file('/embeddings/glove.twitter.27B.200d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.17313 , -0.083534,  0.094943,  0.27862 , -0.09849 , -0.64505 ,\n",
       "       -0.034571, -0.033253, -0.14127 ,  0.77595 , -0.50909 ,  0.48752 ,\n",
       "       -0.11292 ,  0.12668 ,  0.43857 ,  0.25239 ,  0.034386, -0.19116 ,\n",
       "        0.1735  , -0.081495,  0.15889 , -0.46626 , -0.036711,  0.30366 ,\n",
       "        0.26433 ,  0.59701 ,  0.45844 , -0.24775 , -0.35584 ,  0.037122,\n",
       "        0.18221 ,  0.46557 , -0.44061 , -0.17489 ,  0.10444 ,  0.42832 ,\n",
       "        0.37302 , -0.017115, -0.33439 , -0.071264,  0.77562 , -0.6526  ,\n",
       "        0.47253 ,  0.32325 ,  0.095131,  0.13935 ,  0.074671,  0.31263 ,\n",
       "       -0.53981 ,  0.041234], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_embeds.embeddings_index.get('pippa')[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = {}\n",
    "tmp.get('james') == None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextData:\n",
    "    def __init__(self, embedding_index, embedding_dim, batch_size=64, buffer_size=10000, output_seq_length=250, max_tokens=10000):\n",
    "        self.batch_size = batch_size\n",
    "        self.buffer_size = buffer_size\n",
    "        self.train_inputs = []\n",
    "        self.train_labels = []\n",
    "        self.test_inputs = []\n",
    "        self.test_labels = []\n",
    "        self.embedding_index = embedding_index\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.output_seq_length = output_seq_length\n",
    "        self.vectorizer = TextVectorization(max_tokens=max_tokens, output_sequence_length=self.output_seq_length)\n",
    "        self.missed_words = []\n",
    "        self.label_index = {}\n",
    "        \n",
    "    def load_from_csv(self, path, delim, has_header, text_col, label_col, array, label):\n",
    "        with open(path) as f:\n",
    "            contents = f.read()\n",
    "            lines = contents.split(\"\\n\")\n",
    "            data = [row.split(delim) for row in lines if row != '']\n",
    "\n",
    "            \n",
    "            if has_header:\n",
    "                header, data = data[0], data[1:]\n",
    "                input_index, label_index = header.index(text_col), header.index(label_col)\n",
    "            else:\n",
    "                input_index, label_index = text_col, label_col\n",
    "            \n",
    "            for row in data:\n",
    "                array.append(row[input_index])\n",
    "                label.append(row[label_index])\n",
    "                \n",
    "    def train_data_csv(self, path, delim, has_header, text_col, label_col):\n",
    "        self.load_from_csv(path, delim, has_header, text_col, label_col, self.train_inputs, self.train_labels)\n",
    "        \n",
    "    def test_data_csv(self, path, delim, has_header, text_col, label_col):\n",
    "        self.load_from_csv(path, delim, has_header, text_col, label_col, self.test_inputs, self.test_labels)\n",
    "    \n",
    "    def auto_encode_labels(self):\n",
    "        counter = 0\n",
    "        current_index = 0\n",
    "        for label in self.train_labels:\n",
    "            if self.label_index.get(label) == None:\n",
    "                self.label_index[label] = counter\n",
    "                counter += 1\n",
    "            self.train_labels[current_index] = self.label_index.get(label)\n",
    "            current_index +=1\n",
    "        \n",
    "                \n",
    "                \n",
    "    \n",
    "    def create_training_tensors(self):\n",
    "        self.training_data = tf.data.Dataset.from_tensor_slices((self.train_inputs, self.train_labels)).shuffle(self.buffer_size).batch(self.batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "        self.testing_data = tf.data.Dataset.from_tensor_slices((self.test_inputs, self.test_labels)).batch(self.batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    def create_embedding_matrix(self):\n",
    "        voc = self.vectorizer.get_vocabulary()\n",
    "        hits = 0\n",
    "        misses = 0\n",
    "        num_tokens = len(voc) + 2\n",
    "        self.embedding_matrix = np.zeros((num_tokens, self.embedding_dim))\n",
    "        for word, i in self.word_index.items():\n",
    "            embedding_vector = self.embedding_index.get(word)\n",
    "            if embedding_vector is not None:\n",
    "                self.embedding_matrix[i] = embedding_vector\n",
    "                hits += 1\n",
    "            else:\n",
    "                self.missed_words.append(word)\n",
    "                misses += 1\n",
    "                \n",
    "        self.embedding_layer = Embedding(\n",
    "            num_tokens,\n",
    "            self.embedding_dim,\n",
    "            embeddings_initializer=keras.initializers.Constant(self.embedding_matrix),\n",
    "            trainable=False\n",
    "        )\n",
    "        \n",
    "        print(\"converted {} words ({} misses)\".format(hits, misses))\n",
    "                \n",
    "    def brew(self):\n",
    "        if len(self.train_inputs) > 0:\n",
    "            self.auto_encode_labels()\n",
    "            self.create_training_tensors()\n",
    "            self.vectorizer.adapt(self.training_data.map(lambda text, label: text))\n",
    "            voc = self.vectorizer.get_vocabulary()\n",
    "            self.word_index = dict(zip(voc, range(len(voc))))\n",
    "            self.create_embedding_matrix()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = TextData(pre_embeds.embeddings_index, pre_embeds.dim)\n",
    "text.train_data_csv('/data/joy_train.txt', delim='\\t', has_header=True, text_col='Tweet', label_col='Intensity Class')\n",
    "text.test_data_csv('/data/joy_test.txt', delim='\\t', has_header=True, text_col='Tweet', label_col='Intensity Class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training samples: 1616\n",
      "number of testing samples:  290\n"
     ]
    }
   ],
   "source": [
    "print(\"number of training samples: {}\".format(len(text.train_inputs)))\n",
    "print(\"number of testing samples:  {}\".format(len(text.test_inputs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converted 4208 words (1629 misses)\n"
     ]
    }
   ],
   "source": [
    "text.brew()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "def uncompiled_model(embedding_matrix, embedding_dim, num_tokens, num_classes):\n",
    "    model = keras.Sequential([\n",
    "        Embedding(num_tokens, embedding_dim, embeddings_initializer=keras.initializers.Constant(embedding_matrix), trainable=False),\n",
    "        layers.Conv1D(128, 5, activation='relu'),\n",
    "        layers.MaxPooling1D(5),\n",
    "        layers.Conv1D(128, 5, activation=\"relu\"),\n",
    "        layers.MaxPooling1D(5),\n",
    "        layers.Conv1D(128, 5, activation=\"relu\"),\n",
    "        layers.GlobalMaxPooling1D(),\n",
    "        layers.Dense(128, activation=\"relu\"),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = uncompiled_model(text.embedding_matrix, text.embedding_dim, len(text.vectorizer.get_vocabulary()) + 2, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train = text.vectorizer(np.array([s for s in text.train_inputs])).numpy()\n",
    "# x_val = text.vectorizer(np.array([s for s in text.train_labels])).numpy()\n",
    "\n",
    "x_train = text.vectorizer(np.array([[s] for s in text.train_inputs])).numpy()\n",
    "y_train = np.array(text.train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "13/13 [==============================] - 6s 318ms/step - loss: 1.3657 - acc: 0.3298 - sparse_categorical_accuracy: 0.3298\n",
      "Epoch 2/20\n",
      "13/13 [==============================] - 4s 281ms/step - loss: 1.2829 - acc: 0.4387 - sparse_categorical_accuracy: 0.4387\n",
      "Epoch 3/20\n",
      "13/13 [==============================] - 4s 315ms/step - loss: 1.1394 - acc: 0.5173 - sparse_categorical_accuracy: 0.5173\n",
      "Epoch 4/20\n",
      "13/13 [==============================] - 4s 278ms/step - loss: 1.1182 - acc: 0.5198 - sparse_categorical_accuracy: 0.5198\n",
      "Epoch 5/20\n",
      "13/13 [==============================] - 4s 338ms/step - loss: 0.9682 - acc: 0.5798 - sparse_categorical_accuracy: 0.5798\n",
      "Epoch 6/20\n",
      "13/13 [==============================] - 4s 349ms/step - loss: 0.8851 - acc: 0.6498 - sparse_categorical_accuracy: 0.6498\n",
      "Epoch 7/20\n",
      "13/13 [==============================] - 4s 322ms/step - loss: 0.7729 - acc: 0.6795 - sparse_categorical_accuracy: 0.6795\n",
      "Epoch 8/20\n",
      "13/13 [==============================] - 5s 373ms/step - loss: 0.7322 - acc: 0.7104 - sparse_categorical_accuracy: 0.7104\n",
      "Epoch 9/20\n",
      "13/13 [==============================] - 4s 289ms/step - loss: 0.5919 - acc: 0.7679 - sparse_categorical_accuracy: 0.7679\n",
      "Epoch 10/20\n",
      "13/13 [==============================] - 4s 291ms/step - loss: 0.5290 - acc: 0.7915 - sparse_categorical_accuracy: 0.7915\n",
      "Epoch 11/20\n",
      "13/13 [==============================] - 4s 342ms/step - loss: 0.4203 - acc: 0.8540 - sparse_categorical_accuracy: 0.8540\n",
      "Epoch 12/20\n",
      "13/13 [==============================] - 4s 295ms/step - loss: 0.3532 - acc: 0.8880 - sparse_categorical_accuracy: 0.8880\n",
      "Epoch 13/20\n",
      "13/13 [==============================] - 5s 386ms/step - loss: 0.2398 - acc: 0.9220 - sparse_categorical_accuracy: 0.9220\n",
      "Epoch 14/20\n",
      "13/13 [==============================] - 5s 401ms/step - loss: 0.2762 - acc: 0.9295 - sparse_categorical_accuracy: 0.9295\n",
      "Epoch 15/20\n",
      "13/13 [==============================] - 5s 383ms/step - loss: 0.3392 - acc: 0.8868 - sparse_categorical_accuracy: 0.8868\n",
      "Epoch 16/20\n",
      "13/13 [==============================] - 4s 340ms/step - loss: 0.1279 - acc: 0.9629 - sparse_categorical_accuracy: 0.9629\n",
      "Epoch 17/20\n",
      "13/13 [==============================] - 4s 292ms/step - loss: 0.2133 - acc: 0.9443 - sparse_categorical_accuracy: 0.9443\n",
      "Epoch 18/20\n",
      "13/13 [==============================] - 5s 410ms/step - loss: 0.0966 - acc: 0.9722 - sparse_categorical_accuracy: 0.9722\n",
      "Epoch 19/20\n",
      "13/13 [==============================] - 4s 315ms/step - loss: 0.5909 - acc: 0.8787 - sparse_categorical_accuracy: 0.8787\n",
      "Epoch 20/20\n",
      "13/13 [==============================] - 6s 441ms/step - loss: 0.0831 - acc: 0.9752 - sparse_categorical_accuracy: 0.9752\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f6850465c88>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\", optimizer=\"rmsprop\", metrics=[\"acc\", keras.metrics.SparseCategoricalAccuracy()]\n",
    ")\n",
    "model.fit(x_train, y_train, batch_size=128, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converted 3 words (2 misses)\n"
     ]
    }
   ],
   "source": [
    "string_input = keras.Input(shape=(1,), dtype=\"string\")\n",
    "text2 = TextData(pre_embeds.embeddings_index, pre_embeds.dim)\n",
    "\n",
    "text2.train_inputs = [\"what is this\"]\n",
    "text2.train_labels = [1]\n",
    "text2.brew()\n",
    "text2.missed_words\n",
    "x = text2.vectorizer(string_input)\n",
    "preds = model(x)\n",
    "end_to_end_model = keras.Model(string_input, preds)\n",
    "# x = text.vectorizer(string_input)\n",
    "# preds = model(x)\n",
    "# end_to_end_model = keras.Model(string_input, preds)\n",
    "\n",
    "# probabilities = end_to_end_model.predict(\n",
    "#     [[\"This day seems to be going just okay\"]]\n",
    "# )\n",
    "\n",
    "# np.argmax(probabilities[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.49349999,  0.35698   ,  0.66068   , ...,  0.17705999,\n",
       "        -0.53694999, -0.29699001],\n",
       "       ...,\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ]])"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 250), dtype=int64, numpy=\n",
       "array([[  5, 107,   3, 783,  17,  34,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0]])>"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.vectorizer([['I need to clean this up']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: ((None,), (None,)), types: (tf.string, tf.string)>"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.create_training_tensors()\n",
    "text.training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "texts:\n",
      " [b'@StephaliciousD afternoon delight'\n",
      " b\"Your lion's heart\\\\nWill protect you under stormy skies\\\\nAnd I will always be listening for your laughter and your tears\"]\n"
     ]
    }
   ],
   "source": [
    "for example, label in text.training_data.take(1):\n",
    "    print('texts:\\n', example.numpy()[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '[UNK]',\n",
       " 'the',\n",
       " 'to',\n",
       " 'a',\n",
       " 'i',\n",
       " 'and',\n",
       " 'you',\n",
       " 'is',\n",
       " 'of',\n",
       " 'in',\n",
       " 'that',\n",
       " 'be',\n",
       " 'my',\n",
       " 'so',\n",
       " 'it',\n",
       " 'for',\n",
       " 'this',\n",
       " 'me',\n",
       " 'with',\n",
       " 'on',\n",
       " 'happy',\n",
       " 'your',\n",
       " 'at',\n",
       " 'but',\n",
       " 'im',\n",
       " 'just',\n",
       " 'lively',\n",
       " 'its',\n",
       " 'have',\n",
       " 'love',\n",
       " 'not',\n",
       " 'by',\n",
       " 'was',\n",
       " 'up',\n",
       " 'amp',\n",
       " 'smile',\n",
       " 'day',\n",
       " 'will',\n",
       " 'all',\n",
       " 'hilarious',\n",
       " 'good',\n",
       " 'as',\n",
       " 'are',\n",
       " 'watch',\n",
       " 'when',\n",
       " 'optimism',\n",
       " 'like',\n",
       " 'if',\n",
       " 'amazing',\n",
       " 'about',\n",
       " 'can',\n",
       " 'he',\n",
       " 'from',\n",
       " 'laughter',\n",
       " 'make',\n",
       " 'out',\n",
       " 'more',\n",
       " 'we',\n",
       " 'glee',\n",
       " 'dont',\n",
       " 'her',\n",
       " 'broadcast',\n",
       " 'musically',\n",
       " 'his',\n",
       " 'get',\n",
       " 'or',\n",
       " 'time',\n",
       " 'see',\n",
       " 'what',\n",
       " 'they',\n",
       " 'life',\n",
       " 'know',\n",
       " 'one',\n",
       " 'do',\n",
       " 'smiling',\n",
       " 'how',\n",
       " 'always',\n",
       " 'now',\n",
       " 'cheer',\n",
       " 'she',\n",
       " 'new',\n",
       " 'an',\n",
       " 'some',\n",
       " 'much',\n",
       " 'today',\n",
       " 'because',\n",
       " 'want',\n",
       " 'over',\n",
       " 'who',\n",
       " 'u',\n",
       " 'has',\n",
       " 'feel',\n",
       " 'our',\n",
       " 'joyful',\n",
       " 'rejoice',\n",
       " 'delight',\n",
       " 'youre',\n",
       " 'still',\n",
       " 'sparkling',\n",
       " 'people',\n",
       " 'got',\n",
       " 'cheerful',\n",
       " 'breezy',\n",
       " 'pleasing',\n",
       " 'no',\n",
       " 'cheering',\n",
       " 'need',\n",
       " 'hilarity',\n",
       " 'great',\n",
       " 'animated',\n",
       " 'why',\n",
       " 'go',\n",
       " 'being',\n",
       " 'back',\n",
       " 'am',\n",
       " 'would',\n",
       " 'way',\n",
       " 'rejoicing',\n",
       " 'playful',\n",
       " 'makes',\n",
       " 'had',\n",
       " 'elated',\n",
       " 'us',\n",
       " 'hearty',\n",
       " 'exhilarating',\n",
       " 'bright',\n",
       " 'were',\n",
       " 'there',\n",
       " 'than',\n",
       " 'lol',\n",
       " 'chirp',\n",
       " 'cheery',\n",
       " 'cant',\n",
       " 'their',\n",
       " 'most',\n",
       " 'joyous',\n",
       " 'best',\n",
       " 'too',\n",
       " 'them',\n",
       " 'never',\n",
       " 'think',\n",
       " 'thank',\n",
       " 'man',\n",
       " 'him',\n",
       " 'did',\n",
       " 'cheerfully',\n",
       " 'thats',\n",
       " 'little',\n",
       " 'levity',\n",
       " 'give',\n",
       " 'every',\n",
       " 'birthday',\n",
       " 'better',\n",
       " 'been',\n",
       " 'such',\n",
       " 'something',\n",
       " 'really',\n",
       " 'well',\n",
       " 'look',\n",
       " 'everything',\n",
       " 'could',\n",
       " 'watching',\n",
       " 'then',\n",
       " 'night',\n",
       " 'ive',\n",
       " 'going',\n",
       " 'everyone',\n",
       " 'ever',\n",
       " 'thanks',\n",
       " 'show',\n",
       " 'off',\n",
       " 'morning',\n",
       " 'after',\n",
       " 'world',\n",
       " 'someone',\n",
       " 'right',\n",
       " 'jovial',\n",
       " 'fun',\n",
       " 'find',\n",
       " 'very',\n",
       " 'only',\n",
       " 'n',\n",
       " 'god',\n",
       " 'come',\n",
       " 'any',\n",
       " '2',\n",
       " 'things',\n",
       " 'team',\n",
       " 'say',\n",
       " 'into',\n",
       " 'exhilaration',\n",
       " 'even',\n",
       " 'beautiful',\n",
       " 'accept',\n",
       " 'week',\n",
       " 'thing',\n",
       " 'mirth',\n",
       " 'may',\n",
       " 'least',\n",
       " 'keep',\n",
       " 'heyday',\n",
       " 'happiness',\n",
       " 'gleeful',\n",
       " 'work',\n",
       " 'wonderful',\n",
       " 'tomorrow',\n",
       " 'said',\n",
       " 'hope',\n",
       " 'here',\n",
       " 'heart',\n",
       " 'face',\n",
       " 'down',\n",
       " 'while',\n",
       " 'victory',\n",
       " 'those',\n",
       " 'sure',\n",
       " 'side',\n",
       " 'mind',\n",
       " 'joy',\n",
       " 'getting',\n",
       " 'funny',\n",
       " 'challenges',\n",
       " 'also',\n",
       " 'again',\n",
       " '😂',\n",
       " 'yet',\n",
       " 'water',\n",
       " 'through',\n",
       " 'looks',\n",
       " 'looking',\n",
       " 'long',\n",
       " 'live',\n",
       " 'last',\n",
       " 'hes',\n",
       " 'gbbo',\n",
       " 'follow',\n",
       " 'done',\n",
       " 'days',\n",
       " 'before',\n",
       " 'bad',\n",
       " 'tonight',\n",
       " 'theres',\n",
       " 'tell',\n",
       " 'stop',\n",
       " 'started',\n",
       " 'since',\n",
       " 'oh',\n",
       " 'nice',\n",
       " 'maybe',\n",
       " 'let',\n",
       " 'laughing',\n",
       " 'first',\n",
       " 'didnt',\n",
       " 'big',\n",
       " 'young',\n",
       " 'remember',\n",
       " 'once',\n",
       " 'laugh',\n",
       " 'home',\n",
       " 'having',\n",
       " 'girl',\n",
       " 'future',\n",
       " 'end',\n",
       " 'year',\n",
       " 'which',\n",
       " 'try',\n",
       " 'thought',\n",
       " 'theyre',\n",
       " 'same',\n",
       " 'quote',\n",
       " 'place',\n",
       " 'other',\n",
       " 'myself',\n",
       " 'made',\n",
       " 'lots',\n",
       " 'lot',\n",
       " 'ill',\n",
       " 'feeling',\n",
       " 'actually',\n",
       " '15',\n",
       " 'youve',\n",
       " 'yourself',\n",
       " 'wanna',\n",
       " 'wait',\n",
       " 'took',\n",
       " 'tears',\n",
       " 'sweet',\n",
       " 'singing',\n",
       " 'should',\n",
       " 's',\n",
       " 'person',\n",
       " 'others',\n",
       " 'next',\n",
       " 'head',\n",
       " 'hard',\n",
       " 'gets',\n",
       " 'fucking',\n",
       " 'friend',\n",
       " 'episode',\n",
       " 'around',\n",
       " 'wish',\n",
       " 'whats',\n",
       " 'twitter',\n",
       " 'these',\n",
       " 'thenicebot',\n",
       " 'take',\n",
       " 'success',\n",
       " 'own',\n",
       " 'nd',\n",
       " 'many',\n",
       " 'lost',\n",
       " 'grateful',\n",
       " 'glad',\n",
       " 'gave',\n",
       " 'free',\n",
       " 'following',\n",
       " 'fans',\n",
       " 'change',\n",
       " 'bit',\n",
       " '3',\n",
       " 'without',\n",
       " 'welcome',\n",
       " 'watched',\n",
       " 'wake',\n",
       " 'used',\n",
       " 'ur',\n",
       " 'together',\n",
       " 'tired',\n",
       " 'spry',\n",
       " 'shes',\n",
       " 'seen',\n",
       " 'seeing',\n",
       " 'saw',\n",
       " 'sad',\n",
       " 'please',\n",
       " 'play',\n",
       " 'outside',\n",
       " 'old',\n",
       " 'ok',\n",
       " 'must',\n",
       " 'miss',\n",
       " 'making',\n",
       " 'lord',\n",
       " 'literally',\n",
       " 'jaunty',\n",
       " 'hey',\n",
       " 'game',\n",
       " 'food',\n",
       " 'far',\n",
       " 'each',\n",
       " 'during',\n",
       " 'class',\n",
       " 'cheerfulness',\n",
       " 'call',\n",
       " 'balls',\n",
       " 'another',\n",
       " 'aint',\n",
       " 'years',\n",
       " 'x',\n",
       " 'working',\n",
       " 'whole',\n",
       " 'where',\n",
       " 'voice',\n",
       " 'use',\n",
       " 'trump',\n",
       " 'truly',\n",
       " 'top',\n",
       " 'talk',\n",
       " 'short',\n",
       " 'real',\n",
       " 'put',\n",
       " 'punny',\n",
       " 'pun',\n",
       " 'proud',\n",
       " 'open',\n",
       " 'nothing',\n",
       " 'news',\n",
       " 'moment',\n",
       " 'met',\n",
       " 'means',\n",
       " 'living',\n",
       " 'juggle',\n",
       " 'jubilant',\n",
       " 'hell',\n",
       " 'hear',\n",
       " 'hate',\n",
       " 'half',\n",
       " 'guys',\n",
       " 'goals',\n",
       " 'friends',\n",
       " 'excited',\n",
       " 'doesnt',\n",
       " 'both',\n",
       " 'blessed',\n",
       " 'awesome',\n",
       " 'against',\n",
       " '7',\n",
       " '☺️',\n",
       " 'yall',\n",
       " 'woman',\n",
       " 'video',\n",
       " 'until',\n",
       " 'tweet',\n",
       " 'tv',\n",
       " 'trying',\n",
       " 'truth',\n",
       " 'though',\n",
       " 'tho',\n",
       " 'talking',\n",
       " 'taking',\n",
       " 'strong',\n",
       " 'start',\n",
       " 'sometimes',\n",
       " 'sick',\n",
       " 'shit',\n",
       " 'series',\n",
       " 'quite',\n",
       " 'problems',\n",
       " 'positive',\n",
       " 'part',\n",
       " 'north',\n",
       " 'lovely',\n",
       " 'listening',\n",
       " 'lips',\n",
       " 'lets',\n",
       " 'job',\n",
       " 'id',\n",
       " 'high',\n",
       " 'help',\n",
       " 'found',\n",
       " 'forward',\n",
       " 'family',\n",
       " 'fact',\n",
       " 'experience',\n",
       " 'evening',\n",
       " 'enough',\n",
       " 'easy',\n",
       " 'door',\n",
       " 'doing',\n",
       " 'deserve',\n",
       " 'crying',\n",
       " 'city',\n",
       " 'buoyant',\n",
       " 'bring',\n",
       " 'boy',\n",
       " 'book',\n",
       " 'bet',\n",
       " 'behappy',\n",
       " 'anything',\n",
       " 'anyone',\n",
       " 'almost',\n",
       " 'ago',\n",
       " '😂😂',\n",
       " '❤️',\n",
       " 'worst',\n",
       " 'words',\n",
       " 'word',\n",
       " 'wonder',\n",
       " 'white',\n",
       " 'two',\n",
       " 'treat',\n",
       " 'thy',\n",
       " 'super',\n",
       " 'stuff',\n",
       " 'stay',\n",
       " 'spot',\n",
       " 'spirit',\n",
       " 'somebody',\n",
       " 'socialmedia',\n",
       " 'simply',\n",
       " 'send',\n",
       " 'saying',\n",
       " 'ready',\n",
       " 'read',\n",
       " 'race',\n",
       " 'promise',\n",
       " 'point',\n",
       " 'playing',\n",
       " 'player',\n",
       " 'phone',\n",
       " 'past',\n",
       " 'movie',\n",
       " 'missed',\n",
       " 'mighty',\n",
       " 'mean',\n",
       " 'lucky',\n",
       " 'light',\n",
       " 'kurt',\n",
       " 'kind',\n",
       " 'incredible',\n",
       " 'important',\n",
       " 'gym',\n",
       " 'goes',\n",
       " 'goal',\n",
       " 'gift',\n",
       " 'gen',\n",
       " 'fuck',\n",
       " 'film',\n",
       " 'few',\n",
       " 'felt',\n",
       " 'fall',\n",
       " 'enjoy',\n",
       " 'due',\n",
       " 'draw',\n",
       " 'different',\n",
       " 'death',\n",
       " 'damn',\n",
       " 'cry',\n",
       " 'coming',\n",
       " 'comes',\n",
       " 'care',\n",
       " 'bro',\n",
       " 'break',\n",
       " 'black',\n",
       " 'bill',\n",
       " 'bc',\n",
       " 'ass',\n",
       " 'arms',\n",
       " 'arent',\n",
       " 'ages',\n",
       " 'afternoon',\n",
       " 'aesthetically',\n",
       " 'act',\n",
       " '25',\n",
       " '—',\n",
       " 'youll',\n",
       " 'yo',\n",
       " 'yesterday',\n",
       " 'yeah',\n",
       " 'ya',\n",
       " 'wont',\n",
       " 'wolf',\n",
       " 'within',\n",
       " 'weve',\n",
       " 'went',\n",
       " 'weeks',\n",
       " 'w',\n",
       " 'v',\n",
       " 'totally',\n",
       " 'thankyou',\n",
       " 'survivor',\n",
       " 'soul',\n",
       " 'soon',\n",
       " 'sit',\n",
       " 'share',\n",
       " 'seem',\n",
       " 'second',\n",
       " 'season',\n",
       " 'school',\n",
       " 'rest',\n",
       " 'reason',\n",
       " 'pure',\n",
       " 'problemsolving',\n",
       " 'present',\n",
       " 'positivity',\n",
       " 'picture',\n",
       " 'omg',\n",
       " 'okay',\n",
       " 'negative',\n",
       " 'needs',\n",
       " 'nawaz',\n",
       " 'name',\n",
       " 'music',\n",
       " 'mood',\n",
       " 'mom',\n",
       " 'minutes',\n",
       " 'might',\n",
       " 'merriment',\n",
       " 'memories',\n",
       " 'meet',\n",
       " 'medicine',\n",
       " 'lmao',\n",
       " 'liveliness',\n",
       " 'listen',\n",
       " 'lie',\n",
       " 'less',\n",
       " 'king',\n",
       " 'kindness',\n",
       " 'kids',\n",
       " 'isnt',\n",
       " 'instead',\n",
       " 'individual',\n",
       " 'imagine',\n",
       " 'house',\n",
       " 'history',\n",
       " 'happier',\n",
       " 'hair',\n",
       " 'gladness',\n",
       " 'george',\n",
       " 'full',\n",
       " 'four',\n",
       " 'forgot',\n",
       " 'fine',\n",
       " 'finally',\n",
       " 'feels',\n",
       " 'extra',\n",
       " 'evil',\n",
       " 'everyday',\n",
       " 'either',\n",
       " 'eating',\n",
       " 'early',\n",
       " 'dude',\n",
       " 'double',\n",
       " 'does',\n",
       " 'definitely',\n",
       " 'decide',\n",
       " 'cuz',\n",
       " 'cute',\n",
       " 'cut',\n",
       " 'coys',\n",
       " 'country',\n",
       " 'cool',\n",
       " 'content',\n",
       " 'confident',\n",
       " 'coffee',\n",
       " 'church',\n",
       " 'check',\n",
       " 'charlotte',\n",
       " 'challenge',\n",
       " 'called',\n",
       " 'bunch',\n",
       " 'brought',\n",
       " 'brilliant',\n",
       " 'breakfast',\n",
       " 'boys',\n",
       " 'blithe',\n",
       " 'bf',\n",
       " 'begin',\n",
       " 'bed',\n",
       " 'became',\n",
       " 'bday',\n",
       " 'awkward',\n",
       " 'awful',\n",
       " 'away',\n",
       " 'attempt',\n",
       " 'asked',\n",
       " 'american',\n",
       " 'already',\n",
       " 'along',\n",
       " 'air',\n",
       " 'age',\n",
       " '5',\n",
       " '1st',\n",
       " '…',\n",
       " 'yoga',\n",
       " 'yes',\n",
       " 'xmas',\n",
       " 'wrong',\n",
       " 'worship',\n",
       " 'worse',\n",
       " 'winning',\n",
       " 'wine',\n",
       " 'whether',\n",
       " 'welll',\n",
       " 'wednesday',\n",
       " 'wch2016',\n",
       " 'wasnt',\n",
       " 'wanted',\n",
       " 'vs',\n",
       " 'version',\n",
       " 'val',\n",
       " 'using',\n",
       " 'upon',\n",
       " 'united',\n",
       " 'ukulele',\n",
       " 'tweets',\n",
       " 'tumblr',\n",
       " 'trust',\n",
       " 'tried',\n",
       " 'toward',\n",
       " 'told',\n",
       " 'tip',\n",
       " 'times',\n",
       " 'thisisus',\n",
       " 'thee',\n",
       " 'talkin',\n",
       " 'supposed',\n",
       " 'support',\n",
       " 'story',\n",
       " 'store',\n",
       " 'starts',\n",
       " 'spent',\n",
       " 'sound',\n",
       " 'song',\n",
       " 'social',\n",
       " 'snapchat',\n",
       " 'smiles',\n",
       " 'sleep',\n",
       " 'situation',\n",
       " 'single',\n",
       " 'signed',\n",
       " 'shower',\n",
       " 'sharpened',\n",
       " 'sharif',\n",
       " 'service',\n",
       " 'self',\n",
       " 'seems',\n",
       " 'secret',\n",
       " 'says',\n",
       " 'save',\n",
       " 'sadness',\n",
       " 'room',\n",
       " 'rojo',\n",
       " 'red',\n",
       " 'realize',\n",
       " 'reality',\n",
       " 'racism',\n",
       " 'r',\n",
       " 'quotesoup',\n",
       " 'possible',\n",
       " 'pleasure',\n",
       " 'plays',\n",
       " 'photos',\n",
       " 'phil',\n",
       " 'patton',\n",
       " 'party',\n",
       " 'order',\n",
       " 'needed',\n",
       " 'nation',\n",
       " 'msnbc',\n",
       " 'month',\n",
       " 'misery',\n",
       " 'minute',\n",
       " 'mine',\n",
       " 'match',\n",
       " 'mad',\n",
       " 'loving',\n",
       " 'loves',\n",
       " 'loved',\n",
       " 'lives',\n",
       " 'letting',\n",
       " 'leave',\n",
       " 'learning',\n",
       " 'later',\n",
       " 'known',\n",
       " 'kinda',\n",
       " 'key',\n",
       " 'keeping',\n",
       " 'join',\n",
       " 'italian',\n",
       " 'inspire',\n",
       " 'hot',\n",
       " 'hoping',\n",
       " 'hit',\n",
       " 'hearing',\n",
       " 'heard',\n",
       " 'health',\n",
       " 'havent',\n",
       " 'hasnt',\n",
       " 'happened',\n",
       " 'hall',\n",
       " 'green',\n",
       " 'greatest',\n",
       " 'goodday',\n",
       " 'gonna',\n",
       " 'given',\n",
       " 'girlfriends',\n",
       " 'football',\n",
       " 'fight',\n",
       " 'fear',\n",
       " 'fat',\n",
       " 'fallen',\n",
       " 'fails',\n",
       " 'eyes',\n",
       " 'expect',\n",
       " 'everybody',\n",
       " 'endless',\n",
       " 'eat',\n",
       " 'dumb',\n",
       " 'drink',\n",
       " 'dream',\n",
       " 'direction',\n",
       " 'died',\n",
       " 'die',\n",
       " 'describe',\n",
       " 'decision',\n",
       " 'd',\n",
       " 'course',\n",
       " 'company',\n",
       " 'comedy',\n",
       " 'close',\n",
       " 'clear',\n",
       " 'clean',\n",
       " 'choir',\n",
       " 'children',\n",
       " 'changing',\n",
       " 'cannot',\n",
       " 'came',\n",
       " 'calm',\n",
       " 'buy',\n",
       " 'bout',\n",
       " 'bored',\n",
       " 'books',\n",
       " 'body',\n",
       " 'blood',\n",
       " 'bizhour',\n",
       " 'birds',\n",
       " 'beyond',\n",
       " 'believe',\n",
       " 'behind',\n",
       " 'beauty',\n",
       " 'bag',\n",
       " 'baby',\n",
       " 'b',\n",
       " 'attitude',\n",
       " 'ask',\n",
       " 'apart',\n",
       " 'angry',\n",
       " 'absolutely',\n",
       " '70s',\n",
       " '317',\n",
       " '🤔',\n",
       " '🙄',\n",
       " '😘',\n",
       " '😐',\n",
       " '😍',\n",
       " '🎉',\n",
       " '✨',\n",
       " '♥',\n",
       " '“set',\n",
       " 'yr',\n",
       " 'youtube',\n",
       " 'yourebeautiful',\n",
       " 'yep',\n",
       " 'yeg',\n",
       " 'ye',\n",
       " 'wsjnordics',\n",
       " 'wow',\n",
       " 'worry',\n",
       " 'women',\n",
       " 'wishing',\n",
       " 'wished',\n",
       " 'wink',\n",
       " 'wicked',\n",
       " 'whose',\n",
       " 'weekend',\n",
       " 'wearing',\n",
       " 'weakness',\n",
       " 'ways',\n",
       " 'walking',\n",
       " 'vote',\n",
       " 'visit',\n",
       " 'ustinov',\n",
       " 'under',\n",
       " 'uk',\n",
       " 'turn',\n",
       " 'true',\n",
       " 'trans',\n",
       " 'tour',\n",
       " 'tone',\n",
       " 'thursday',\n",
       " 'thumbs',\n",
       " 'three',\n",
       " 'thinking',\n",
       " 'thin',\n",
       " 'theyll',\n",
       " 'terribledebatequestions',\n",
       " 'teamnawch',\n",
       " 'tag',\n",
       " 'sydneyswans',\n",
       " 'swim',\n",
       " 'sweden',\n",
       " 'sugar',\n",
       " 'successful',\n",
       " 'stupid',\n",
       " 'stretch',\n",
       " 'strength',\n",
       " 'staystrong',\n",
       " 'state',\n",
       " 'stand',\n",
       " 'sprightly',\n",
       " 'sports',\n",
       " 'spiritual',\n",
       " 'speechless',\n",
       " 'someones',\n",
       " 'smart',\n",
       " 'sir',\n",
       " 'silence',\n",
       " 'shut',\n",
       " 'shows',\n",
       " 'showed',\n",
       " 'shitty',\n",
       " 'sharing',\n",
       " 'set',\n",
       " 'seriously',\n",
       " 'serious',\n",
       " 'score',\n",
       " 'scifi',\n",
       " 'scene',\n",
       " 'ryan',\n",
       " 'russian',\n",
       " 'round',\n",
       " 'root',\n",
       " 'romance',\n",
       " 'rn',\n",
       " 'ring',\n",
       " 'ride',\n",
       " 'rewatching',\n",
       " 'respect',\n",
       " 'replace',\n",
       " 'reminds',\n",
       " 'reinervshunter',\n",
       " 'reflection',\n",
       " 'recipe',\n",
       " 'recent',\n",
       " 'reading',\n",
       " 'raw',\n",
       " 'ps',\n",
       " 'provide',\n",
       " 'proverbs',\n",
       " 'prices',\n",
       " 'pretty',\n",
       " 'premiere',\n",
       " 'prayer',\n",
       " 'praise',\n",
       " 'practice',\n",
       " 'policy',\n",
       " 'plus',\n",
       " 'players',\n",
       " 'pizza',\n",
       " 'pity',\n",
       " 'pile',\n",
       " 'piece',\n",
       " 'photographer',\n",
       " 'peter',\n",
       " 'perfume',\n",
       " 'peace',\n",
       " 'paper',\n",
       " 'pakistan',\n",
       " 'pak',\n",
       " 'pain',\n",
       " 'p',\n",
       " 'overtime',\n",
       " 'ought',\n",
       " 'otherwise',\n",
       " 'opening',\n",
       " 'online',\n",
       " 'ones',\n",
       " 'often',\n",
       " 'office',\n",
       " 'offers',\n",
       " 'odds',\n",
       " 'occasion',\n",
       " 'obama',\n",
       " 'numbers',\n",
       " 'normal',\n",
       " 'noise',\n",
       " 'nickofferman',\n",
       " 'nhlstorenyc',\n",
       " 'nflonfox',\n",
       " 'national',\n",
       " 'na',\n",
       " 'mustacheharbor',\n",
       " 'moon',\n",
       " 'money',\n",
       " 'modicum',\n",
       " 'min',\n",
       " 'midst',\n",
       " 'miami',\n",
       " 'method',\n",
       " 'men',\n",
       " 'meeting',\n",
       " 'loud',\n",
       " 'lose',\n",
       " 'lmfao',\n",
       " 'lil',\n",
       " 'liking',\n",
       " 'level',\n",
       " 'lethalweapon',\n",
       " 'leaning',\n",
       " 'leads',\n",
       " 'laughs',\n",
       " 'laughed',\n",
       " 'late',\n",
       " 'lady',\n",
       " 'ladies',\n",
       " 'kiss',\n",
       " 'killed',\n",
       " 'kick',\n",
       " 'keeps',\n",
       " 'kanye',\n",
       " 'justice',\n",
       " 'joysnnquote',\n",
       " 'joviality',\n",
       " 'jobs',\n",
       " 'jesus',\n",
       " 'jeffprobst',\n",
       " 'jeep',\n",
       " 'itnfunny',\n",
       " 'itn',\n",
       " 'itll',\n",
       " 'indeed',\n",
       " 'incredibly',\n",
       " 'hurt',\n",
       " 'hungry',\n",
       " 'hun',\n",
       " 'human',\n",
       " 'hopefully',\n",
       " 'holiday',\n",
       " ...]"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.vectorizer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
